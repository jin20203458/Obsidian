- 마르코프 체인 + 보상
**마르코프 보상 프로세스 (Markov Reward Process, MRP)는 
마르코프 결정 과정 (Markov Decision Process, MDP)의 한 부분으로, 확률적 상태 전이와 보상 시스템**을 포함하는 모델입니다. 주로 **강화 학습**과 관련하여 사용되며, 에이전트가 환경과 상호작용하면서 상태를 전이하고 보상을 얻는 과정을 모델링합니다.

마르코프 보상 프로세스는 **상태(state)**, **전이 확률(transition probability)**, **보상(reward)** 세 가지 주요 요소로 구성됩니다.

### **마르코프 보상 프로세스의 구성 요소**

1. **상태 (State, S)**  
    각 상태는 환경의 특정 상황을 나타냅니다. 예를 들어, 날씨 상태(Sunny, Cloudy, Rainy)나, 로봇이 위치한 공간의 상태일 수 있습니다.
    
2. **전이 확률 (Transition Probability, P)**  
    상태 간 전이는 확률적으로 일어납니다. 상태에서 다른 상태로 전이될 확률을 정의하는 함수입니다. 예를 들어, Sunny에서 Cloudy로 변할 확률, Cloudy에서 Rainy로 변할 확률 등을 말합니다. 일반적으로 **P(s' | s)** 형태로 사용되며, 상태 `s`에서 상태 `s'`로 전이될 확률을 나타냅니다.
    
3. **보상 함수 (Reward Function, R)**  
    각 상태에 대해 주어지는 **보상**을 정의하는 함수입니다. 보상은 에이전트가 특정 상태에 도달했을 때 얻는 값으로, 상태가 좋은지 나쁜지를 평가합니다. 이 보상은 강화 학습에서 **에이전트가 더 나은 행동을 선택할 수 있도록 유도**하는 중요한 요소입니다. 일반적으로 **R(s)로 표현되며, 특정 상태 `s`에서 얻는 보상을 나타냅니다.
    
4. **할인 계수 (Discount Factor, γ)**  
    할인 계수 γ (0 ≤ γ ≤ 1)는 미래의 보상을 얼마나 중요하게 생각할지 결정하는 파라미터입니다. 1에 가까울수록 미래 보상을 현재 보상만큼 중요하게 여깁니다. 0에 가까울수록 미래의 보상은 중요하지 않게 됩니다.